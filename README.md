# ğŸš€ Spur AI Live Chat Agent

ğŸŒ **Live Demo:** [https://spur-ai-chat-zeta.vercel.app](https://spur-ai-chat-zeta.vercel.app)  
ğŸ“¦ **Backend API:** [https://spur-ai-chat-backend-08ew.onrender.com](https://spur-ai-chat-backend-08ew.onrender.com)

---

## ğŸ“– Overview

This project is a small but realistic AI-powered customer support chat system built as part of the Spur founding engineer take-home assignment.

It simulates a live support chat widget where an AI agent answers user questions about a fictional e-commerce store using a real LLM, with conversation persistence and robust error handling.

The focus is on:
- Correctness
- Robustness
- Clean architecture
- Product-quality UX

---

## ğŸ§± Tech Stack

### Backend
- Node.js
- TypeScript
- Express
- Prisma ORM
- SQLite
- Groq SDK (LLaMA 3.1)

### Frontend
- Svelte (Vite)
- Fetch API
- Vanilla CSS

---

## ğŸ› ï¸ Local Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone [https://github.com/huzaifa1097/spur-ai-chat.git](https://github.com/huzaifa1097/spur-ai-chat.git)
cd spur-ai-chat
```

### ğŸ”§ Backend Setup

**2ï¸âƒ£ Navigate to backend directory**
```bash
cd backend
```

**3ï¸âƒ£ Install backend dependencies**
```bash
npm install
```

**4ï¸âƒ£ Configure environment variables**
Create a `.env` file inside the `backend/` directory:

```env
GROQ_API_KEY=your_groq_api_key_here
```
> âš ï¸ **Note:** Do NOT commit this file. It is already gitignored.

**5ï¸âƒ£ Set up the database (Prisma)**
```bash
npx prisma migrate dev --name init
```
This will:
- Create the SQLite database
- Apply schema migrations
- Generate the Prisma client

*(Optional) Inspect the database:*
```bash
npx prisma studio
```

**6ï¸âƒ£ Start the backend server**
```bash
npx ts-node-dev --respawn --transpile-only src/index.ts
```
Backend runs at: `http://localhost:3001`

### ğŸ¨ Frontend Setup

**7ï¸âƒ£ Navigate to frontend directory**
```bash
cd ../frontend
```

**8ï¸âƒ£ Install frontend dependencies**
```bash
npm install
```

**9ï¸âƒ£ Configure backend API URL (local development)**
Open the file: `frontend/src/lib/api.ts`

Set the API URL to:
```ts
const API_URL = "http://localhost:3001/chat/message";
```

**ğŸ”Ÿ Start the frontend development server**
```bash
npm run dev
```
Frontend runs at: `http://localhost:5173`

---

## ğŸ“¡ API Documentation

### POST `/chat/message`

**Request**
```json
{
  "message": "Do you ship to USA?",
  "sessionId": "optional-session-id"
}
```

**Response**
```json
{
  "reply": "Yes, we ship worldwide. Delivery to the USA takes 5â€“7 days.",
  "sessionId": "generated-or-existing-session-id"
}
```

---

## ğŸ§ª Example API Test Cases (Postman / cURL)

### âœ… Normal message
```json
{
  "message": "What is your return policy?"
}
```

### âœ… Context continuation
```json
{
  "message": "How long does it take?",
  "sessionId": "<existing-session-id>"
}
```

### âŒ Empty message (validation)
```json
{
  "message": ""
}
```
**Expected:** `400 Bad Request`

### âŒ Invalid body
```json
{
  "foo": "bar"
}
```
**Expected:** `400 Bad Request`

### âš ï¸ LLM failure handling
If the LLM API key is invalid or rate-limited, the backend returns:
```json
{
  "reply": "Sorry, something went wrong. Please try again.",
  "sessionId": "<session-id>"
}
```

---

## ğŸ§  Architecture Overview

### Backend Structure
```bash
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ routes/        # Express routes
â”‚   â”œâ”€â”€ services/      # Business logic (chat, LLM)
â”‚   â”œâ”€â”€ db/            # Prisma client
â”‚   â””â”€â”€ index.ts       # Server entry point
```

### Design Choices
- **Routes** are thin and focused on HTTP.
- **Services** contain all core logic.
- **LLM integration** is isolated for easy replacement.
- **Database schema** supports future multi-channel expansion.

---

## ğŸ¤– LLM Integration

- **Provider:** Groq
- **Model:** llama-3.1-8b-instant

### Prompting Strategy
The AI is seeded with domain knowledge for a fictional ecommerce store:
- Shipping policy
- Returns and refunds
- Support hours

Conversation history is included so replies remain contextual.

### Guardrails
- Invalid input rejected early
- LLM/network errors handled gracefully
- Friendly fallback responses
- Token usage capped for predictability

---

## ğŸ—‚ï¸ Data Model

**Conversation**
- `id`
- `createdAt`

**Message**
- `id`
- `conversationId`
- `sender` (user | ai)
- `text`
- `createdAt`

All messages are persisted and associated with a conversation.

---

## ğŸ›¡ï¸ Robustness & Validation

- Empty messages return `400 Bad Request`
- Very long messages do not crash the server
- Backend never crashes on malformed input
- LLM/API failures surfaced gracefully
- No secrets are hard-coded

---

## âš–ï¸ Trade-offs & Future Improvements

### Trade-offs
- SQLite used locally for simplicity (Postgres-ready schema).
- Frontend does not auto-reload chat history on refresh.

### If I had more timeâ€¦
- Add `GET /chat/history/:sessionId`
- Streaming responses for typing effect
- Rate limiting per session
- Multi-channel adapters (WhatsApp, Instagram)

---

## ğŸŒ Deployed URLs

- **Frontend:** [https://spur-ai-chat-zeta.vercel.app](https://spur-ai-chat-zeta.vercel.app)
- **Backend:** [https://spur-ai-chat-backend-08ew.onrender.com](https://spur-ai-chat-backend-08ew.onrender.com)

---

## ğŸ‘¤ About Me

**Abu Huzaifa Ahmad**

ğŸŒ **Portfolio:** [https://huzaifa-portfolio-seven.vercel.app](https://huzaifa-portfolio-seven.vercel.app)  
ğŸ’¼ **LinkedIn:** [https://www.linkedin.com/in/abu-huzaifa-ahmad-68175222a](https://www.linkedin.com/in/abu-huzaifa-ahmad-68175222a)  
ğŸ’» **GitHub:** [https://github.com/huzaifa1097](https://github.com/huzaifa1097)  
ğŸ“§ **Email:** [ahmadhuzaifa1097@gmail.com](mailto:ahmadhuzaifa1097@gmail.com)

ğŸ™ Thank you for reviewing this submission. Happy to walk through any part of the implementation.
